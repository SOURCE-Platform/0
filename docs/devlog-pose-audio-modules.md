# Development Log: Pose Estimation & Audio Processing Modules

**Date:** 2025-11-18
**Feature:** Phase 2.5 - Pose Estimation and Audio Processing Integration
**Status:** Core Architecture Implemented âœ“

---

## Overview

This development log documents the implementation of two major feature sets for the "0" application:
1. **Pose Estimation Module**: Real-time body and facial tracking with maximum keypoint capture
2. **Audio Processing Module**: Comprehensive audio capture, transcription, speaker diarization, and emotion detection

Both modules are designed to function independently while integrating seamlessly with the existing session-based recording system.

---

## Architecture Design

### Design Principles

1. **Modularity**: Each component (pose tracking, transcription, diarization, emotion detection) can function independently
2. **Composability**: Modules can be combined for comprehensive user activity analysis
3. **Privacy-First**: Integration with existing consent management system
4. **Database-Driven**: All data stored in SQLite for offline-first functionality
5. **Real-time Feedback**: Event streaming for live updates to frontend

### Technology Stack

**Pose Estimation:**
- **MediaPipe** (Google): Industry-leading pose and face tracking
  - Face Mesh: 468 3D facial landmarks
  - Pose: 33 body keypoints
  - Hands: 21 hand keypoints per hand
  - ARKit-compatible 52 facial blendshapes for expression analysis

**Audio Processing:**
- **OpenAI Whisper**: State-of-the-art speech transcription (99 languages)
- **pyannote.audio**: Speaker diarization ("who spoke when")
- **Demucs v4** (Meta AI): Audio source separation (vocals, music, bass, other)
- **SpeechBrain**: Speech emotion recognition (7 emotions + valence/arousal)

**Integration:**
- Rust backend with PyO3 for Python ML model integration
- Alternative: ONNX Runtime for pure Rust inference (future optimization)

---

## Implementation Details

### 1. Data Models (`src-tauri/src/models/`)

#### Pose Models (`pose.rs`)

Created comprehensive data structures for pose tracking:

```rust
pub struct PoseFrame {
    pub session_id: String,
    pub timestamp: i64,
    pub body_pose: Option<BodyPose>,      // 33 keypoints
    pub face_mesh: Option<FaceMesh>,      // 468 landmarks + 52 blendshapes
    pub hands: Vec<HandPose>,             // 21 keypoints per hand
    pub processing_time_ms: u64,
}
```

**Key Features:**
- 3D keypoints with confidence scores
- ARKit-compatible facial blendshapes for expression analysis
- Pose classification (standing, sitting, lying, etc.)
- World landmarks for metric 3D coordinates
- Expression intensity calculation from blendshapes

#### Audio Models (`audio.rs`)

Created data structures for comprehensive audio processing:

```rust
pub struct AudioRecording {
    pub id: String,
    pub session_id: String,
    pub sample_rate: u32,
    pub channels: u16,
    pub codec: AudioCodec,
    // ...
}

pub struct TranscriptSegment {
    pub text: String,
    pub language: String,
    pub speaker_id: Option<String>,
    pub words: Vec<WordTimestamp>,
    // ...
}
```

**Key Components:**
- Audio device abstraction (microphone, system loopback)
- Source separation metadata (vocals, music, bass, other)
- Speaker segments with voice embeddings (512-dim)
- Emotion results with valence and arousal
- Full-text search integration via SQLite FTS5

### 2. Database Schema (`src-tauri/migrations/`)

#### Pose Tables (`20251118000001_create_pose_tables.sql`)

```sql
-- Stores raw pose data (33 body + 468 face + 42 hand keypoints per frame)
CREATE TABLE pose_frames (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    timestamp INTEGER NOT NULL,
    body_keypoints_json TEXT,      -- 33 body landmarks
    face_landmarks_json TEXT,      -- 468 facial landmarks
    face_blendshapes_json TEXT,    -- 52 expression coefficients
    left_hand_json TEXT,           -- 21 left hand landmarks
    right_hand_json TEXT,          -- 21 right hand landmarks
    processing_time_ms INTEGER,
    FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
);

-- Aggregated facial expression events
CREATE TABLE facial_expressions (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    expression_type TEXT NOT NULL,  -- smile, frown, surprised, etc.
    intensity REAL NOT NULL,
    duration_ms INTEGER,
    blendshapes_json TEXT,
    FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
);
```

**Indices:**
- `idx_pose_session`: Fast session-based queries
- `idx_pose_timestamp`: Temporal range queries
- `idx_expressions_type`: Expression filtering

#### Audio Tables (`20251118000002_create_audio_tables.sql`)

```sql
-- Audio recording sessions
CREATE TABLE audio_recordings (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    sample_rate INTEGER NOT NULL,
    channels INTEGER NOT NULL,
    file_path TEXT NOT NULL,
    codec TEXT NOT NULL,
    FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
);

-- Source separation metadata
CREATE TABLE audio_sources (
    id TEXT PRIMARY KEY,
    recording_id TEXT NOT NULL,
    has_user_speech INTEGER NOT NULL,
    has_system_audio INTEGER NOT NULL,
    system_audio_type TEXT,        -- music, video, game, etc.
    vocals_file_path TEXT,
    music_file_path TEXT,
    FOREIGN KEY (recording_id) REFERENCES audio_recordings(id) ON DELETE CASCADE
);

-- Speech transcriptions with full-text search
CREATE TABLE transcripts (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    text TEXT NOT NULL,
    language TEXT NOT NULL,
    confidence REAL NOT NULL,
    speaker_id TEXT,
    words_json TEXT,               -- Word-level timestamps
    FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
);

CREATE VIRTUAL TABLE transcripts_fts USING fts5(
    text,
    content='transcripts'
);

-- Speaker diarization
CREATE TABLE speaker_segments (
    id TEXT PRIMARY KEY,
    recording_id TEXT NOT NULL,
    speaker_id TEXT NOT NULL,      -- SPEAKER_00, SPEAKER_01, etc.
    start_timestamp INTEGER NOT NULL,
    end_timestamp INTEGER NOT NULL,
    embedding_json TEXT,           -- 512-dim voice embedding
    FOREIGN KEY (recording_id) REFERENCES audio_recordings(id) ON DELETE CASCADE
);

-- Emotion detection
CREATE TABLE emotion_detections (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    emotion TEXT NOT NULL,         -- happy, sad, angry, neutral, etc.
    confidence REAL NOT NULL,
    valence REAL NOT NULL,         -- -1.0 to 1.0 (negative to positive)
    arousal REAL NOT NULL,         -- 0.0 to 1.0 (calm to excited)
    FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
);
```

**Indices:**
- `idx_transcripts_session`, `idx_transcripts_speaker`: Fast filtering
- `idx_speakers_recording`, `idx_speakers_id`: Diarization queries
- `idx_emotions_session`, `idx_emotions_type`: Emotion analytics

### 3. Core Modules (`src-tauri/src/core/`)

#### Pose Detector (`pose_detector.rs`)

**Responsibilities:**
- Coordinate pose tracking lifecycle
- Process frames through MediaPipe
- Store pose frames and facial expressions
- Calculate pose statistics

**Key Methods:**
```rust
pub async fn start_tracking(&self, session_id: String, config: PoseConfig) -> PoseResult<()>
pub async fn process_frame(&self, frame_data: &[u8], width: u32, height: u32, timestamp: i64) -> PoseResult<()>
pub async fn get_pose_frames(&self, session_id: &str, start: i64, end: i64) -> PoseResult<Vec<PoseFrameDto>>
pub async fn get_pose_statistics(&self, session_id: &str) -> PoseResult<PoseStatistics>
```

**Features:**
- Async frame processing pipeline
- Background task for database storage
- Expression classification from blendshapes
- Configurable model complexity (lite/full/heavy)
- Target FPS configuration (default: 15 fps)

#### Audio Recorder (`audio_recorder.rs`)

**Responsibilities:**
- Manage audio capture from microphone and system loopback
- Coordinate audio file storage
- Track recording sessions

**Key Methods:**
```rust
pub async fn start_recording(&self, session_id: String, config: AudioConfig) -> AudioResult<String>
pub async fn stop_recording(&self) -> AudioResult<()>
pub async fn get_devices() -> AudioResult<Vec<AudioDevice>>
pub async fn get_session_recordings(&self, session_id: &str) -> AudioResult<Vec<AudioRecording>>
```

**Configuration:**
- Sample rate (default: 48000 Hz)
- Channels (mono/stereo)
- Codec (WAV, AAC, MP3, Opus)
- Device selection (microphone, system audio, both)

#### Speech Transcriber (`speech_transcriber.rs`)

**Responsibilities:**
- Transcribe audio files using Whisper
- Store transcripts with word-level timestamps
- Full-text search integration

**Key Methods:**
```rust
pub async fn transcribe_audio(&self, session_id: &str, recording_id: &str, audio_file_path: &str) -> AudioResult<Vec<TranscriptSegment>>
pub async fn get_transcripts(&self, session_id: &str, start: i64, end: i64, speaker_id: Option<String>) -> AudioResult<Vec<TranscriptSegment>>
pub async fn search_transcripts(&self, query: &str, session_id: Option<String>) -> AudioResult<Vec<TranscriptSegment>>
```

**Whisper Model Sizes:**
- Tiny: ~39M params (fastest, least accurate)
- Base: ~74M params (fast, decent)
- Small: ~244M params (balanced)
- Medium: ~769M params (slower, more accurate)
- Large/Large-v3: ~1550M params (slowest, best quality)

#### Speaker Diarizer (`speaker_diarizer.rs`)

**Responsibilities:**
- Identify "who spoke when" using pyannote.audio
- Extract voice embeddings for speaker identification
- Aggregate speaker statistics

**Key Methods:**
```rust
pub async fn diarize_audio(&self, recording_id: &str, audio_file_path: &str) -> AudioResult<Vec<SpeakerSegment>>
pub async fn get_speakers(&self, session_id: &str) -> AudioResult<Vec<SpeakerInfo>>
pub async fn get_speaker_segments(&self, recording_id: &str, speaker_id: Option<String>) -> AudioResult<Vec<SpeakerSegment>>
```

**Features:**
- Voice embedding clustering (512-dim vectors)
- Automatic speaker count detection
- Primary user identification (most speaking time)
- Segment confidence scores

#### Emotion Detector (`emotion_detector.rs`)

**Responsibilities:**
- Detect emotions in speech using SpeechBrain
- Calculate valence (positive/negative) and arousal (intensity)
- Aggregate emotion statistics

**Key Methods:**
```rust
pub async fn detect_emotions(&self, session_id: &str, recording_id: &str, audio_file_path: &str) -> AudioResult<Vec<EmotionResult>>
pub async fn get_emotions(&self, session_id: &str, start: i64, end: i64, speaker_id: Option<String>, emotion_type: Option<String>) -> AudioResult<Vec<EmotionResult>>
pub async fn get_emotion_statistics(&self, session_id: &str, speaker_id: Option<String>) -> AudioResult<EmotionStatistics>
```

**Emotion Categories:**
- Neutral
- Happy
- Sad
- Angry
- Fearful
- Surprised
- Disgusted

**Additional Metrics:**
- Valence: -1.0 (negative) to 1.0 (positive)
- Arousal: 0.0 (calm) to 1.0 (excited)

---

## Data Flow Architecture

### Pose Estimation Pipeline

```
Screen Capture (RawFrame)
    â†“
PoseDetector::process_frame()
    â†“
MediaPipe Inference (TODO: PyO3 bridge)
    â”œâ”€â†’ Body Pose (33 keypoints)
    â”œâ”€â†’ Face Mesh (468 landmarks + 52 blendshapes)
    â””â”€â†’ Hands (21 keypoints Ã— 2)
    â†“
Background Processing Task
    â”œâ”€â†’ Store in pose_frames table
    â”œâ”€â†’ Classify facial expression
    â””â”€â†’ Store in facial_expressions table
    â†“
Tauri Event Emission (real-time updates)
```

### Audio Processing Pipeline

```
Audio Capture (Microphone + System Loopback)
    â†“
AudioRecorder::start_recording()
    â†“
Raw Audio File (WAV/AAC/MP3)
    â†“
    â”œâ”€â†’ Source Separation (Demucs)
    â”‚       â”œâ”€â†’ Vocals (user speech)
    â”‚       â”œâ”€â†’ Music/instrumental
    â”‚       â”œâ”€â†’ Bass
    â”‚       â””â”€â†’ Other (SFX, ambient)
    â”‚
    â”œâ”€â†’ Speech Transcription (Whisper)
    â”‚       â”œâ”€â†’ Text segments
    â”‚       â”œâ”€â†’ Language detection
    â”‚       â”œâ”€â†’ Word timestamps
    â”‚       â””â”€â†’ Full-text search index
    â”‚
    â”œâ”€â†’ Speaker Diarization (pyannote.audio)
    â”‚       â”œâ”€â†’ Speaker segments
    â”‚       â”œâ”€â†’ Voice embeddings
    â”‚       â””â”€â†’ Speaker attribution
    â”‚
    â””â”€â†’ Emotion Detection (SpeechBrain)
            â”œâ”€â†’ Emotion classification
            â”œâ”€â†’ Valence/arousal scores
            â””â”€â†’ Timestamp association
```

---

## Module Integration

### Session Association

All modules integrate with the existing session management system:

```rust
pub struct Session {
    pub id: String,
    pub start_timestamp: i64,
    pub end_timestamp: Option<i64>,
    // Existing fields...
    pub pose_tracking_enabled: bool,      // NEW
    pub audio_recording_enabled: bool,    // NEW
}
```

### Unified Recording Command (Future)

```rust
#[tauri::command]
async fn start_unified_recording(config: UnifiedRecordingConfig) -> Result<String, String> {
    let session_id = session_manager.create_session().await?;

    if config.enable_screen_recording {
        screen_recorder.start(session_id.clone()).await?;
    }
    if config.enable_pose_tracking {
        pose_detector.start(session_id.clone()).await?;
    }
    if config.enable_audio_recording {
        audio_recorder.start(session_id.clone()).await?;
    }

    Ok(session_id)
}
```

### Consent Management

Both modules integrate with the existing consent system:

```rust
pub enum ConsentFeature {
    // Existing features...
    PoseTracking,           // NEW
    AudioRecording,         // NEW
    SpeechTranscription,    // NEW
    EmotionDetection,       // NEW
}
```

---

## Implementation Status

### âœ“ Completed

1. **Architecture Design**
   - [x] Comprehensive module design document
   - [x] Data flow diagrams
   - [x] Technology stack selection
   - [x] Integration strategy

2. **Data Models**
   - [x] Pose estimation models (`models/pose.rs`)
   - [x] Audio processing models (`models/audio.rs`)
   - [x] Error types and result wrappers
   - [x] DTO structures for Tauri commands

3. **Database Schema**
   - [x] Pose frames table with indices
   - [x] Facial expressions table
   - [x] Audio recordings table
   - [x] Transcripts table with FTS5 search
   - [x] Speaker segments table
   - [x] Emotion detections table
   - [x] Audio sources table
   - [x] All necessary indices and foreign keys

4. **Core Modules**
   - [x] PoseDetector with async processing pipeline
   - [x] AudioRecorder with device management
   - [x] SpeechTranscriber with FTS integration
   - [x] SpeakerDiarizer with voice embeddings
   - [x] EmotionDetector with statistics aggregation

5. **Module Registration**
   - [x] Added to `src-tauri/src/core/mod.rs`
   - [x] Added to `src-tauri/src/models/mod.rs`

6. **Documentation**
   - [x] Architecture document (`docs/pose-audio-modules-architecture.md`)
   - [x] Development log (this file)
   - [x] Python requirements file

### ðŸš§ Pending Implementation

1. **MediaPipe Integration**
   - [ ] PyO3 bridge for Python MediaPipe
   - [ ] Alternative: ONNX Runtime with MediaPipe models
   - [ ] Frame preprocessing pipeline
   - [ ] Model loading and caching

2. **Audio Capture**
   - [ ] Platform-specific audio device enumeration (cpal)
   - [ ] Microphone capture
   - [ ] System loopback audio (macOS: Core Audio, Windows: WASAPI, Linux: PulseAudio)
   - [ ] Audio encoding with FFmpeg

3. **ML Model Integration**
   - [ ] Whisper integration (openai-whisper or faster-whisper)
   - [ ] Demucs source separation
   - [ ] pyannote.audio diarization
   - [ ] SpeechBrain emotion recognition
   - [ ] Model downloading and caching
   - [ ] GPU acceleration support

4. **Tauri Commands**
   - [ ] Pose tracking commands
   - [ ] Audio recording commands
   - [ ] Transcription query commands
   - [ ] Diarization query commands
   - [ ] Emotion query commands
   - [ ] Add to `invoke_handler` in `lib.rs`

5. **Frontend Integration**
   - [ ] TypeScript types for pose/audio DTOs
   - [ ] Real-time event listeners
   - [ ] Pose visualization components
   - [ ] Audio waveform display
   - [ ] Transcript viewer
   - [ ] Emotion timeline visualization

6. **Testing**
   - [ ] Unit tests for data models
   - [ ] Integration tests for database operations
   - [ ] End-to-end tests for recording pipeline
   - [ ] Performance benchmarks

7. **Performance Optimization**
   - [ ] Frame rate optimization for pose tracking
   - [ ] GPU acceleration for ML models
   - [ ] Batch processing for transcription
   - [ ] Memory usage optimization
   - [ ] Storage compression

---

## Technical Challenges & Solutions

### Challenge 1: Real-time Pose Tracking Performance

**Problem:** MediaPipe inference can be computationally expensive, potentially blocking the main thread.

**Solution:**
- Async processing pipeline with mpsc channels
- Frame buffering and skip logic
- Configurable target FPS (default: 15 fps)
- GPU acceleration via ONNX Runtime
- Lower resolution input (640x480 sufficient for MediaPipe)

### Challenge 2: Audio Source Separation

**Problem:** Differentiating between system audio and microphone input.

**Solution:**
- Dual capture: Microphone + System Loopback
- Demucs for source separation into vocals/music/bass/other
- Classification heuristics (speech probability, music probability)
- Metadata storage for downstream processing

### Challenge 3: Speaker Diarization Accuracy

**Problem:** Identifying individual speakers without labeled training data.

**Solution:**
- pyannote.audio pre-trained models
- Voice embedding clustering (512-dim vectors)
- Confidence scores for each segment
- Primary user identification based on speaking time
- Manual speaker labeling (future feature)

### Challenge 4: Python-Rust Integration

**Problem:** ML models are primarily Python-based, but backend is Rust.

**Solutions (in order of preference):**
1. **ONNX Runtime**: Convert models to ONNX for pure Rust inference
   - Pros: No Python dependency, faster, easier deployment
   - Cons: Not all models available in ONNX format
2. **PyO3**: Embed Python interpreter in Rust
   - Pros: Full access to Python ML ecosystem
   - Cons: Additional dependency, GIL contention
3. **HTTP API**: Separate Python service
   - Pros: Language agnostic, scalable
   - Cons: Network overhead, deployment complexity

### Challenge 5: Storage Requirements

**Problem:** Pose data (~216 MB/hour) + Audio (~660 MB/hour) + Separated sources (~240 MB/hour) = ~1.1 GB/hour.

**Solutions:**
- Configurable compression levels
- Selective recording (motion-triggered for pose)
- Periodic cleanup of old data
- Cloud backup options (future)
- User storage quota warnings

---

## Performance Estimates

### Pose Tracking
- **Target FPS:** 15-30 fps
- **Processing Time:** ~30-50ms per frame (CPU), ~10-20ms (GPU)
- **Storage:** ~2 KB per frame â†’ ~216 MB/hour @ 30 fps
- **CPU Usage:** 10-20% (one core) @ 15 fps

### Audio Processing
- **Transcription:** ~0.3x realtime (Whisper base), ~0.1x (Whisper tiny)
- **Diarization:** ~0.5x realtime (pyannote.audio)
- **Emotion Detection:** ~0.2x realtime (SpeechBrain)
- **Source Separation:** ~0.1x realtime (Demucs GPU), ~2x (CPU)
- **Storage:** ~60 MB/hour (compressed audio) + ~1-5 MB (transcripts)

### System Requirements
- **Minimum:** 8 GB RAM, 4-core CPU, 20 GB storage
- **Recommended:** 16 GB RAM, 8-core CPU + GPU, SSD, 100 GB storage
- **Optimal:** 32 GB RAM, NVIDIA GPU (CUDA 11.8+) or Apple Silicon, NVMe SSD

---

## Future Enhancements

### Phase 3.0: Advanced Features
1. **Activity Recognition**: Detect typing, mouse movements, speaking
2. **Context-Aware Summarization**: "You smiled 15 times during this meeting"
3. **Posture Analysis**: Ergonomic recommendations
4. **Voice Analysis**: Pitch, tone, speaking rate trends
5. **Multi-Modal Correlation**: Link facial expressions with speech emotions

### Phase 3.1: Privacy & Security
1. **Local-only Processing**: No cloud dependencies
2. **Selective Redaction**: Auto-blur sensitive facial expressions
3. **Encryption at Rest**: Database encryption
4. **Granular Consent**: Per-feature, per-session consent
5. **Data Retention Policies**: Auto-delete after N days

### Phase 3.2: ML Model Optimization
1. **Model Quantization**: INT8/FP16 for faster inference
2. **Model Distillation**: Smaller models with comparable accuracy
3. **Edge Deployment**: On-device inference without cloud
4. **Custom Fine-Tuning**: User-specific emotion models
5. **Federated Learning**: Improve models without sharing data

---

## Known Limitations

1. **MediaPipe Integration:** Placeholder implementation - requires PyO3 or ONNX integration
2. **Audio Capture:** Platform-specific code not yet implemented
3. **ML Models:** Inference pipelines are placeholders
4. **Tauri Commands:** Not yet added to `lib.rs` invoke handler
5. **Frontend:** No UI components for pose/audio visualization
6. **Testing:** No comprehensive test coverage yet
7. **Documentation:** API docs need to be generated

---

## Developer Notes

### Building the Project

```bash
# Install Rust dependencies
cargo build

# Install Python dependencies (for ML models)
pip install -r python-requirements.txt

# Download ML models (on first use)
# MediaPipe: Auto-downloaded by MediaPipe library
# Whisper: Auto-downloaded by openai-whisper
# pyannote.audio: Requires HuggingFace token (see docs)
# Demucs: Auto-downloaded by demucs library

# Run the application
npm run tauri dev
```

### Code Organization

```
src-tauri/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                      # Business logic
â”‚   â”‚   â”œâ”€â”€ pose_detector.rs       # Pose tracking orchestrator
â”‚   â”‚   â”œâ”€â”€ audio_recorder.rs      # Audio capture manager
â”‚   â”‚   â”œâ”€â”€ speech_transcriber.rs  # Whisper integration
â”‚   â”‚   â”œâ”€â”€ speaker_diarizer.rs    # pyannote.audio integration
â”‚   â”‚   â””â”€â”€ emotion_detector.rs    # SpeechBrain integration
â”‚   â”œâ”€â”€ models/                    # Data structures
â”‚   â”‚   â”œâ”€â”€ pose.rs                # Pose/face models
â”‚   â”‚   â””â”€â”€ audio.rs               # Audio/transcript models
â”‚   â”œâ”€â”€ platform/                  # OS-specific code (TODO)
â”‚   â”‚   â”œâ”€â”€ pose/                  # MediaPipe bindings
â”‚   â”‚   â””â”€â”€ audio/                 # Audio device APIs
â”‚   â””â”€â”€ lib.rs                     # Tauri app entry point
â”œâ”€â”€ migrations/                    # Database schema
â”‚   â”œâ”€â”€ 20251118000001_create_pose_tables.sql
â”‚   â””â”€â”€ 20251118000002_create_audio_tables.sql
â””â”€â”€ Cargo.toml                     # Rust dependencies

docs/
â”œâ”€â”€ pose-audio-modules-architecture.md    # Detailed architecture
â””â”€â”€ devlog-pose-audio-modules.md          # This file

python-requirements.txt            # Python ML dependencies
```

---

## References

### MediaPipe
- Documentation: https://developers.google.com/mediapipe
- Models: https://github.com/google/mediapipe/tree/master/mediapipe/modules
- Face Mesh: 468 landmarks guide
- Pose: 33 keypoints guide
- Hands: 21 keypoints guide

### OpenAI Whisper
- GitHub: https://github.com/openai/whisper
- Model sizes: https://github.com/openai/whisper#available-models-and-languages
- faster-whisper: https://github.com/guillaumekln/faster-whisper

### pyannote.audio
- Documentation: https://github.com/pyannote/pyannote-audio
- Pretrained models: https://huggingface.co/pyannote
- Speaker diarization guide: https://github.com/pyannote/pyannote-audio/blob/develop/tutorials

### Demucs
- GitHub: https://github.com/facebookresearch/demucs
- Model weights: https://github.com/facebookresearch/demucs/blob/main/docs/training.md

### SpeechBrain
- Documentation: https://speechbrain.github.io/
- Emotion recognition: https://huggingface.co/speechbrain/emotion-recognition-wav2vec2-IEMOCAP

---

## Conclusion

This phase establishes the foundational architecture for pose estimation and audio processing in the "0" application. The core data models, database schema, and module structure are complete and ready for ML model integration.

**Next Steps:**
1. Implement MediaPipe integration (PyO3 or ONNX)
2. Implement audio capture with platform-specific APIs
3. Integrate Whisper, Demucs, pyannote.audio, and SpeechBrain
4. Add Tauri commands to expose functionality to frontend
5. Build React components for visualization
6. Performance testing and optimization
7. User testing and feedback

**Total Lines of Code Added:** ~3,500 lines
**Files Created:** 11 files (models, core modules, migrations, docs)
**Estimated Completion:** Phase 2.5 architecture: 100% âœ“ | Phase 2.5 implementation: 40% ðŸš§

---

**Author:** Claude (Anthropic AI)
**Project:** 0 - Privacy-First Activity Tracker
**Phase:** 2.5 - Pose & Audio Integration
**Last Updated:** 2025-11-18
